{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Tiny Lego dataset\n",
    "- https://people.eecs.berkeley.edu/~bmild/nerf/tiny_nerf_data.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"tiny_nerf_data.npz\"):\n",
    "  !wget https://people.eecs.berkeley.edu/~bmild/nerf/tiny_nerf_data.npz\n",
    "\n",
    "dataset = np.load(\"tiny_nerf_data.npz\")\n",
    "print(dataset[\"images\"].shape)\n",
    "print(dataset[\"poses\"].shape)\n",
    "print(dataset['focal'])\n",
    "fig, axs = plt.subplots(nrows=1, ncols=10, figsize=(30,4))\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    plt.sca(ax)\n",
    "    plt.imshow(dataset['images'][i])\n",
    "    plt.title('Image: {}'.format(i+1))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading (Pinhole Camera Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses = dataset['poses']\n",
    "dirs = np.stack([np.sum([0, 0, -1] * pose[:3, :3], axis=-1) for pose in poses])\n",
    "origins = poses[:, :3, 3]\n",
    "\n",
    "ax = plt.figure(figsize=(12,8)).add_subplot(projection=\"3d\")\n",
    "_ = ax.quiver(\n",
    "    origins[..., 0].flatten(),\n",
    "    origins[..., 1].flatten(),\n",
    "    origins[..., 2].flatten(),\n",
    "    dirs[..., 0].flatten(),\n",
    "    dirs[..., 1].flatten(),\n",
    "    dirs[..., 2].flatten(), length=0.5, normalize=True\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rays(h: int, w: int, focal_length: float, pose: torch.Tensor):\n",
    "  i, j = torch.meshgrid(\n",
    "      torch.arange(w, dtype=torch.float32).to(pose),\n",
    "      torch.arange(h, dtype=torch.float32).to(pose),\n",
    "      indexing='ij')\n",
    "  i, j = i.transpose(-1, -2), j.transpose(-1, -2)\n",
    "  rays_d = torch.stack([(i - w * .5) / focal_length,\n",
    "                            -(j - h * .5) / focal_length,\n",
    "                            -torch.ones_like(i)\n",
    "                           ], dim=-1)\n",
    "  rays_d = torch.sum(rays_d[..., None, :] * pose[:3, :3], dim=-1)\n",
    "  rays_o = pose[:3, -1].expand(rays_d.shape)\n",
    "  return rays_o, rays_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sampling(\n",
    "    rays_o,\n",
    "    rays_d,\n",
    "    near,\n",
    "    far,\n",
    "    n,\n",
    "):\n",
    "  # shape: (num_samples)\n",
    "  t = torch.linspace(near, far, n).to(rays_o)\n",
    "  \n",
    "  # ray_origins: (width, height, 3)\n",
    "  # noise_shape = (width, height, num_samples)\n",
    "  noise_shape = list(rays_o.shape[:-1]) + [n]\n",
    "  \n",
    "  # depth_values: (num_samples)\n",
    "  t = t + torch.rand(noise_shape).to(rays_o) * (far - near) / n\n",
    "  \n",
    "  # (width, height, num_samples, 3) = (width, height, 1, 3) + (width, height, 1, 3) * (num_samples, 1)\n",
    "  # query_points:  (width, height, num_samples, 3)\n",
    "  x = rays_o[..., None, :] + rays_d[..., None, :] * t[..., :, None]\n",
    "  \n",
    "  return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(\n",
    "    x, L=6, include_input=True\n",
    ") -> torch.Tensor:\n",
    "  encoding = [x] if include_input else []\n",
    "  frequency_bands = 2.0 ** torch.linspace(\n",
    "        0.0,\n",
    "        L - 1,\n",
    "        L,\n",
    "        dtype=x.dtype,\n",
    "        device=x.device,\n",
    "  )\n",
    "  for freq in frequency_bands:\n",
    "    encoding.append(torch.sin(x * freq * np.pi))\n",
    "    encoding.append(torch.cos(x * freq * np.pi))\n",
    "  \n",
    "  return torch.cat(encoding, dim=-1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
