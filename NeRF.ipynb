{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Tiny Lego dataset\n",
    "- https://people.eecs.berkeley.edu/~bmild/nerf/tiny_nerf_data.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"tiny_nerf_data.npz\"):\n",
    "  !wget https://people.eecs.berkeley.edu/~bmild/nerf/tiny_nerf_data.npz\n",
    "\n",
    "dataset = np.load(\"tiny_nerf_data.npz\")\n",
    "print(dataset[\"images\"].shape)\n",
    "print(dataset[\"poses\"].shape)\n",
    "print(dataset['focal'])\n",
    "fig, axs = plt.subplots(nrows=1, ncols=10, figsize=(30,4))\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    plt.sca(ax)\n",
    "    plt.imshow(dataset['images'][i])\n",
    "    plt.title('Image: {}'.format(i+1))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading (Pinhole Camera Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses = dataset['poses']\n",
    "dirs = np.stack([np.sum([0, 0, -1] * pose[:3, :3], axis=-1) for pose in poses])\n",
    "origins = poses[:, :3, 3]\n",
    "\n",
    "ax = plt.figure(figsize=(12,8)).add_subplot(projection=\"3d\")\n",
    "_ = ax.quiver(\n",
    "    origins[..., 0].flatten(),\n",
    "    origins[..., 1].flatten(),\n",
    "    origins[..., 2].flatten(),\n",
    "    dirs[..., 0].flatten(),\n",
    "    dirs[..., 1].flatten(),\n",
    "    dirs[..., 2].flatten(), length=0.5, normalize=True\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rays(h: int, w: int, focal_length: float, pose: torch.Tensor):\n",
    "  i, j = torch.meshgrid(\n",
    "      torch.arange(w, dtype=torch.float32).to(pose),\n",
    "      torch.arange(h, dtype=torch.float32).to(pose),\n",
    "      indexing='ij')\n",
    "  i, j = i.transpose(-1, -2), j.transpose(-1, -2)\n",
    "  rays_d = torch.stack([(i - w * .5) / focal_length,\n",
    "                            -(j - h * .5) / focal_length,\n",
    "                            -torch.ones_like(i)\n",
    "                           ], dim=-1)\n",
    "  rays_d = torch.sum(rays_d[..., None, :] * pose[:3, :3], dim=-1)\n",
    "  rays_o = pose[:3, -1].expand(rays_d.shape)\n",
    "  return rays_o, rays_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sampling(\n",
    "    rays_o,\n",
    "    rays_d,\n",
    "    near,\n",
    "    far,\n",
    "    n,\n",
    "):\n",
    "  # shape: (num_samples)\n",
    "  t = torch.linspace(near, far, n).to(rays_o)\n",
    "  \n",
    "  # ray_origins: (width, height, 3)\n",
    "  # noise_shape = (width, height, num_samples)\n",
    "  noise_shape = list(rays_o.shape[:-1]) + [n]\n",
    "  \n",
    "  # depth_values: (num_samples)\n",
    "  t = t + torch.rand(noise_shape).to(rays_o) * (far - near) / n\n",
    "  \n",
    "  # (width, height, num_samples, 3) = (width, height, 1, 3) + (width, height, 1, 3) * (num_samples, 1)\n",
    "  # query_points:  (width, height, num_samples, 3)\n",
    "  x = rays_o[..., None, :] + rays_d[..., None, :] * t[..., :, None]\n",
    "  \n",
    "  return x, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(\n",
    "    x, L=6, include_input=True\n",
    ") -> torch.Tensor:\n",
    "  encoding = [x] if include_input else []\n",
    "  frequency_bands = 2.0 ** torch.linspace(\n",
    "        0.0,\n",
    "        L - 1,\n",
    "        L,\n",
    "        dtype=x.dtype,\n",
    "        device=x.device,\n",
    "  )\n",
    "  for freq in frequency_bands:\n",
    "    encoding.append(torch.sin(x * freq * np.pi))\n",
    "    encoding.append(torch.cos(x * freq * np.pi))\n",
    "  \n",
    "  return torch.cat(encoding, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO 1.\n",
    "def volume_rendering(c, sigma, rays_o, t):\n",
    "  sigma = torch.nn.functional.relu(sigma)[...,0]\n",
    "  c = torch.sigmoid(c)\n",
    "  delta = t[..., 1:] - t[..., :-1]\n",
    "  delta = torch.cat([delta, torch.tensor([1e10], dtype=rays_o.dtype, device=rays_o.device).expand(t[...,:1].shape)], dim=-1)\n",
    "\n",
    "  alpha = 1. - torch.exp(-sigma * delta)\n",
    "  T = torch.cumprod(1. - alpha + 1e-10, -1)\n",
    "  T = torch.roll(T, 1, -1)\n",
    "  T[..., 0] = 1.\n",
    "\n",
    "  w = T * alpha\n",
    "\n",
    "  rgb = (w[..., None] * c).sum(dim=-2)\n",
    "  return rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRF(torch.nn.Module):\n",
    "  def __init__(self, gamma_x_dim=60, gamma_d_dim=24, num_channels=256, num_layers=8, skip=4):\n",
    "    super(NeRF, self).__init__()\n",
    "\n",
    "    self.layers = []\n",
    "    for i in range(num_layers):\n",
    "      if i == 0:\n",
    "        self.layers.append(torch.nn.Linear(gamma_x_dim, num_channels))\n",
    "      elif not i == skip:\n",
    "        self.layers.append(torch.nn.Linear(num_channels, num_channels))\n",
    "      else: \n",
    "        self.layers.append(torch.nn.Linear(num_channels + gamma_x_dim, num_channels))\n",
    "    self.layers = torch.nn.ModuleList(self.layers)\n",
    "    self.sigma_out = torch.nn.Linear(num_channels, 1)\n",
    "\n",
    "    self.feature = torch.nn.Linear(num_channels, num_channels)\n",
    "    self.branch = torch.nn.Linear(num_channels + gamma_d_dim, num_channels // 2)\n",
    "    self.rgb_out = torch.nn.Linear(num_channels // 2, 3)\n",
    "\n",
    "    self.skip = skip\n",
    "    self.relu = torch.nn.functional.relu\n",
    "  \n",
    "  def forward(self, x, d):\n",
    "    out = x\n",
    "    for i, l in enumerate(self.layers):\n",
    "      if not i == self.skip:\n",
    "        out = self.relu(self.layers[i](out))\n",
    "      else:\n",
    "        out = torch.concat([out, x], dim=-1)\n",
    "        out = self.relu(self.layers[i](out))\n",
    "\n",
    "    sigma = self.sigma_out(out)\n",
    "\n",
    "    out = self.feature(out)\n",
    "    out = torch.concat([out, d], dim=-1)\n",
    "    out = self.relu(self.branch(out))\n",
    "    color = self.rgb_out(out)\n",
    "\n",
    "    return color, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "images = dataset['images']\n",
    "poses = dataset['poses']\n",
    "focal_length = dataset['focal']\n",
    "\n",
    "num_images, h, w = images.shape[:3]\n",
    "test_idx = 101\n",
    "test_image = images[test_idx]\n",
    "test_pose = poses[test_idx]\n",
    "\n",
    "images = torch.from_numpy(images[:100, ... ,:3]).to(device)\n",
    "poses = torch.from_numpy(poses).to(device)\n",
    "focal_length = torch.from_numpy(focal_length).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# traing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_x = 10\n",
    "L_d = 4\n",
    "\n",
    "N_c = 32\n",
    "\n",
    "lr = 5e-4\n",
    "num_iters = 10000\n",
    "num_channels = 128\n",
    "num_layers = 8\n",
    "skip = 4\n",
    "\n",
    "display_every = 1000\n",
    "\n",
    "near = 2. \n",
    "far = 6. \n",
    "\n",
    "batch_size = 4 * 1024\n",
    "\n",
    "include_input = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement single training iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(h, w, focal_length, pose, near, far, n_c, L_c, L_d):\n",
    "\n",
    "  rays_o, rays_d = get_rays(h, w, focal_length, pose)\n",
    "\n",
    "  x, t = stratified_sampling(rays_o, rays_d, near, far, n_c)\n",
    "\n",
    "  # (width, height, n_c, 3)\n",
    "  # (widht * height * n_c, 3)\n",
    "  x_flatten = x.reshape(-1, 3)\n",
    "  d_flatten = rays_d[..., None, :].expand_as(x).reshape(-1, 3)\n",
    "\n",
    "  gamma_x = positional_encoding(x_flatten, L_x, include_input)\n",
    "  gamma_d = positional_encoding(d_flatten, L_d, include_input)\n",
    "  \n",
    "  pred = []\n",
    "  for i in range(0, gamma_x.shape[0], batch_size):\n",
    "    pred.append(model(gamma_x[i:i+batch_size], gamma_d[i:i+batch_size]))\n",
    "\n",
    "  colors = torch.concat([p[0] for p in pred], dim=0).reshape(h, w, -1, 3)\n",
    "  sigma = torch.concat([p[1] for p in pred], dim=0).reshape(h, w, -1, 1)\n",
    "  \n",
    "  rgb = volume_rendering(colors, sigma, rays_o, t)\n",
    "  return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_x_dim = 3 * 2 * L_x + (3 if include_input else 0)\n",
    "gamma_d_dim = 3 * 2 * L_d + (3 if include_input else 0)\n",
    "model = NeRF(gamma_x_dim, gamma_d_dim, num_channels, num_layers, skip)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "seed = 9458\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "pbar = tqdm(range(num_iters))\n",
    "for i in pbar:\n",
    "  idx = np.random.randint(images.shape[0])\n",
    "  image_i = images[idx]\n",
    "  pose_i = poses[idx]\n",
    "\n",
    "  rgb_predicted = train_step(h, w, focal_length, pose_i, near, far, N_c, L_x, L_d)\n",
    "  \n",
    "  loss = torch.nn.functional.mse_loss(rgb_predicted, image_i)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "  if i % display_every == 0:\n",
    "    rgb_predicted = train_step(h, w, focal_length, torch.from_numpy(test_pose).to(device), near, far, N_c, L_x, L_d)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(rgb_predicted.detach().cpu().numpy())\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(test_image)\n",
    "    plt.title(f\"Iteration {i}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
